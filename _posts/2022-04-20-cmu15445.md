---

layout: post
title: CMU-15445 Lab 笔记
date: 2022-04-20
tag: 数据库

---



>  官方文档 https://15445.courses.cs.cmu.edu/fall2021

## PROJECT#1 BUFFER POOL



### TASK#1 LRU REPLACEMENT POLICY

作为本次 Lab 的第一个 task，上来就是要我们实现一个 LRU，还好之前在 LeetCode 上实现过类似的，倒也问题不大，双向链表加哈希一把梭

````C++
private:
  // TODO(student): implement me!
  struct frameNode {
    frame_id_t frame_id_{-1};
    struct frameNode *pre{nullptr};
    struct frameNode *next{nullptr};

    explicit frameNode(frame_id_t frame_id) : frame_id_(frame_id){}
  };
  frameNode *dummyHead;
  frameNode *dummyTail;

  size_t capacity_{0};
  size_t size_{0};

  std::mutex lru_latch_;

  std::unordered_map<frame_id_t, frameNode *> frame_map_;

 private:
  // 链表相关操作
  void RemoveNode(frameNode *frame);
  void MovetoTail(frameNode *frame);
  void AppendtoTail(frameNode *frame);
````

不过和常规 LRU 稍微不一样的是，在这里我们需要实现的功能是 ``victim`` `pin`和 `unpin`

`victim` 希望返回一个最近最久未使用的 frame 来做替换。既然我们用双向链表来做的话就很简单了，直接返回表头元素即可

`pin` 的功能很简单，就是把需要的某个 frame 移出我们的 LRU 池（对应的场景就是此时某个线程正在使用这个 frame，我们当然不希望在使用期间这个 frame 被 LRU 调度给替换掉，所以我们主动把这个 frame 移出 LRU，并增加一个 pin 的标记，这样在使用结束后还能被正常移回到 LRU 池中）

`unpin` 则反过来，当这个 frame 的 pin 标志位为 0  的时候，此时已经没有线程在使用它了，我们就把它再次交由 LRU 调度

这部分还不算太难，唯二需要注意的是在做相关操作比如增加/删除元素的时候记得链表、哈希表、`size`需要同步更新，以及在重复 unpin 同一个 frame 的时候不需要做操作（从测试里面发现的，我也觉得很奇怪） 

```C++
void LRUReplacer::Unpin(frame_id_t frame_id) {
  std::lock_guard<std::mutex> lock(lru_latch_);
  // LOG_INFO("unpin frame %d", frame_id);
  if (!frame_map_.count(frame_id)) {
    frameNode *unpin_frame = new frameNode(frame_id);
    frame_map_[frame_id] = unpin_frame;
    AppendtoTail(unpin_frame);
    size_++;
  } else {
    // just do nothing
    // MovetoTail(frame_map[frame_id]);
  }
}
```



### TASK#2 BUFFER POOL MANAGER INSTANCE

在这个 task，我们需要完成一个 buffer pool manager 实例，这部分需要完成的函数就比较多了。

不过函数体里基本都给了比较详细的步骤提示，按着步骤来应该就大差不差了（测试之前是这么想的），结果写完一跑，连本地测试都过不去。。

随着逐渐 fix 代码里存在的问题，我才发现之前对内容的理解还是不太深入

首先就是 page_id 和 frame_id 之间的关系，借用课件里的图

![](https://picgo-1306905554.cos.ap-shanghai.myqcloud.com/image-20220423005946092.png)

![image-20220423005228629](https://picgo-1306905554.cos.ap-shanghai.myqcloud.com/image-20220423005228629.png)

buffer_pool 作为 manager 里的核心容器，每一格都是一个 frame，frame_id 标识 buffer_pool 里的数组项，我们还有一个 free_list 的数组来表示buffer_pool 里面空闲（也就是还没被分配）的 frame_id ; 而 page 的概念则指的是 disk 上面的内容。manager 里面还有一个 page_table 通过哈希表来表示 page_id 和 frame_id 的对应关系（对应关系指的就是此时这个 frame 被这个 page 所占用。因此对于一个到来的 page_id，我们要先去查 page_table，如果查得到就说明此时这个 page 已经被 buffer_pool 装载，直接通过对应的 frame_id 找就行。

然后还有一些其他边边角角的细节问题（比如说返回值以及标志位），这些才是最折磨人的，因为实现说明里面并没有提到这些，需要自己通过测试加上想象去完善。

* 在 `NewPageImp`的时候，取得对应的 frame_id 之后需要先判断一下这个 frame_id 的 `is_dirty`标志位，如果为真的话得先把脏数据执行一次 `WritePage`操作才能继续重定向操作。并且，在初始化标志位的时候，`pin_count`需要置成 1 （其实也很好理解，既然调用了 newpage 那么说明此时肯定是有一个线程需要使用该 page，置成 1 之后返回的 page 才不至于被 LRU 置换掉）
* 对于`UnpinPageImp`，如果在 page_table 里查不到这个 page_id，**我们此时应该返回的是 true**。我能想到的比较合理的解释是：unpin 的目的就是为了提醒 lru_replacer 可以把这个页面置换掉，那如果这个 page 甚至都已经不在 buffer_pool 里面了，是不是在另外一个角度来说也算是 unpin 成功了呢？另外，函数里还带了一个 is_dirty 参数，不能简单地直接赋给 `pages_[frame_id].is_dirty_`，这个想一下就能知道，如果原来就是脏数据但是参数给的是 false 的话，那最后的标志位应该还是 true 的



###  TASK#3 PARALLEL BUFFER POOL MANAGER

这个 task 相比上一个更上一级。单实例只有一把锁，多线程来争这一把锁的话效率上太低了。所以我们需要完成一个有多实例的 manager

这个 task 算是最不用动脑子的了，基本只要调用上一个 task 完成的 api 就行了，只有 NewPage 的时候需要注意起始 index 的问题，在完成 page 的创建之后，`instance_index`需要自增一个，方便下次 new 的话不会又在同一个位置 newpage



```C++
Page *ParallelBufferPoolManager::NewPgImp(page_id_t *page_id) {
  // create new page. We will request page allocation in a round robin manner from the underlying
  // BufferPoolManagerInstances
  // 1.   From a starting index of the BPMIs, call NewPageImpl until either 1) success and return 2) looped around to starting index and return nullptr
  // 2.   Bump the starting index (mod number of instances) to start search at a different BPMI each time this function is called
  Page *page = nullptr;
  size_t index = instance_index_;
  do {
    page = bufferpool_instance[index]->NewPage(page_id);
    if (page != nullptr) {
      break;
    }
    index = (index + 1) % num_instances_;
  } while (index != instance_index_);
  instance_index_ = (instance_index_ + 1) % num_instances_;
  return page;
}
```

记得在析构函数里把创建的 instance 都销毁了

```C++
ParallelBufferPoolManager::~ParallelBufferPoolManager() {
  for (size_t i = 0; i < num_instances_; i++) {
    delete bufferpool_instance[i];
  }
  delete[] bufferpool_instance;
}
```



满分通过

![image-20220422202020048](https://picgo-1306905554.cos.ap-shanghai.myqcloud.com/image-20220422202020048.png)

没想到在 LeaderBoard 上还能排那么前

![img](https://picgo-1306905554.cos.ap-shanghai.myqcloud.com/QOVO00N3~%7BJ%5BW53PFSILX%60W.png)



## PROJECT#2 EXTENDIBLE HASH INDEX

如果说 Project 1 不看课还可以尚且一做的话，Project 2 不看课就是真的做不下去了。起码仅凭实验要求里的那么一小段说明我是不知道这个 extendible hash 讲的是什么

简要来说，我们这里采用的 hash 方式为链式哈希，使用 bucket （桶）这么个东西来存储 page；不同于常规的 chained hashing 那样 hash(key) 不变，指向的 buckets 桶链表可以往下延伸（最坏情况下又会退化成O（n）的遍历）

![image-20220424154858488](https://picgo-1306905554.cos.ap-shanghai.myqcloud.com/image-20220424154858488.png)

这里的 extendible 可扩展性具体指的是通过 mask 这个算子来完成对不同 bucket 的映射（计算方式与子网掩码相同）

![image-20220424155058589](https://picgo-1306905554.cos.ap-shanghai.myqcloud.com/image-20220424155058589.png)

我们维护一个全局的 `global_mask` 以及每个桶都有自己的 `local_mask`，对于一个到来的 hash 值，我们首先用 `global_mask`计算出这个 key 应该被归到哪个桶（注意多个 slot 可以指向同一个 bucket）

![image-20220424155451610](https://picgo-1306905554.cos.ap-shanghai.myqcloud.com/image-20220424155451610.png)

那么当某个 bucket 填充满的时候又会发生什么情况呢？

![image-20220424155635810](https://picgo-1306905554.cos.ap-shanghai.myqcloud.com/image-20220424155635810.png)

这个时候就需要做拆分了。由于‘10’标志位无法容纳下那么多的 key，所以我们需要把这个超限的桶拆分成两个，并且把`local_mask`标志位都加 1（`global_mask`维护的是所有桶 mask 标志位的最大值所以也需要同步加 1）。在 global slot 完成重定向，以及两桶的数据拆分

![image-20220424160033089](https://picgo-1306905554.cos.ap-shanghai.myqcloud.com/image-20220424160033089.png)

此时就能正常完成插入了

注意在实验里面对于 key 的掩码处理其实是从后面的位开始算的（LSB），和图上的是反过来的，这样的话提出的值可以直接当成数组下标来用



### TASK #1 PAGE LAYOUTS

如果想要我们的哈希表做到数据持久化，即断电后数据还能重新恢复，那哈希表的元数据就必须同样也要存储在磁盘中。这个 Task 需要我们完成两个文件

* Hash_table_dirctory_page
* Hash_table_bucket_page

来支持对于 hash table buckets 的文件读写

#### HASH TABLE  DIRECTORY PAGE

这个页面管理的是 hash table 的元数据，包含

![image-20220424160935274](https://picgo-1306905554.cos.ap-shanghai.myqcloud.com/image-20220424160935274.png)

有几个函数是比较需要注意的

* `uint32_t GetSplitImageIndex(uint32_t *bucket_idx*) `

     一开始没看明白这个函数是用来干什么的，但是做到后面桶的分裂和合并的时候就很需要它了。简而言之它的作用就是用来获取当前 bucket 根据当前深度（**注意这时候的深度应该是分裂增加或者合并缩减之前的深度**）对应的 bucket，这个 split_image_bucket 用来存分裂后原 bucket 拆分的键值对以及合并后原 bucket 也要并入到这个 split_image_bucket 里面去

    实现方式也很简单，就是把传入的 bucket_idx 对应 local_depth 的那一位取反就行了

    比如说 0000 1001，深度为 3，那么它的 split_image_index 就是 0000 1101

* `bool CanShrink()`

    判断是否可以缩容；即当所有桶的 local_depth 都比全局 global_depth 小的时候，认为当前桶量出现冗余，可以进行缩容

* `void IncrGlobalDepth()`

    增加全局 global_depth，但是全局深度增加后全局的 depth 数组和 page_id 数组大小也翻了一倍需要进行初始化（其实就是把原来的复制一遍），所以我写了一个 `Grow()`函数来干这个事情

然后其他的就没有什么问题了，这里不需要担心锁的问题（）



#### HASH TABLE BUCKET PAGE

这里就需要实现对应的桶页逻辑

类里面维护了三个数组成员

* occupied_ ：用于判断该 bucket 该位置是否被访问过
* readable_：用于判断该 bucket 该位置此时是否存有数据
* array_：用于存储真正的键值对

```C++
// For more on BUCKET_ARRAY_SIZE see storage/page/hash_table_page_defs.h
  char occupied_[(BUCKET_ARRAY_SIZE - 1) / 8 + 1];
  // 0 if tombstone/brand new (never occupied), 1 otherwise.
  char readable_[(BUCKET_ARRAY_SIZE - 1) / 8 + 1];
  // Do not add any members below array_, as they will overlap.
  MappingType array_[0];
```

其中 occupied_ 在这里的意义我感觉不太大的样子，除了方便判定是否需要继续遍历数组（某个 occupied_ 为 0 没有访问过的话说明在它之后的都没有被访问过）

 array_ 是零长数组，解释参见（https://blog.csdn.net/gatieme/article/details/64131322）初始化的时候在内存里是不占空间的，但是它真正能使用到的空间就是固定的 page 大小减去前面成员变量之后所剩的空间。因为实际上 BUCKET_ARRAY_SIZE 是固定的，为了不使内存溢出，注释也强调了不要在下面再加别的成员了（所以挺困惑我的一点就是既然总的 page 空间大小和数组 size 都定下来了，为什么不直接在初始化的时候就申请那么大的空间呢）

> 更新： 现在我大概理解了。虽然bucket_page 大小是固定的，但是由于 MappingType （std::pair）的类型并不固定，所以大小不一致，这就使得 array_ 的大小不好分配，声明为零长数组的话就可以灵活使用了，算是一个小 trick 吧

这里需要实现的成员函数都还算简单，涉及到比较多的位运算，要仔细一点算好 index 和 offset 以及是置 0 还是置 1，还有是对位进行操作还是对整个字符（也就是 8 位）进行操作

![image-20220425232212377](https://picgo-1306905554.cos.ap-shanghai.myqcloud.com/image-20220425232212377.png)



### TASK # 2 &3 ExtendibleHashTable & CONCURRENCY CONTROL

这里我把任务 2 和任务 3 放在一起了，反正任务 3 就是任务 2 实现的内容进行一个并发的控制，其实也是顺手加个锁的事情（虽然锁的粒度对最后性能影响还是挺大的）

这个部分我们要做的就是把之前实现的 bucket 和 directory 整合到一起，加上复杂的逻辑，实现`Insert`,`GetValue`,`Remove`

听起来虽然很简单，但是实际上里面的逻辑个人感觉是非常复杂的，尤其是对于 Insert 时候桶满分裂的处理和 Remove 之后桶空合并的处理是需要非常细致的

其他的函数略去不表，重点讲讲 `Split`和 `Merge`的操作

拆分页面需要的步骤

* 加表级写锁
* 获取目录和当前 bucket 的数据
* 判断 directory 里面 global_depth  是否需要增加（local depth == global depth），此时除了增加全局深度，其他的数组也要扩容，参见上面的 `Grow()`。然后当前 bucket 的 depth 也要自增
* 新建一个 bucket，index 就是上面的 splitimageindex，深度什么的和原来的桶一致
* 遍历一遍原来的 bucket，获取原 bucket 的所有 kv pair ，然后再把原 bucket 重置一遍
* 最后把 copy 到的所有 kv pair 重新算一遍 index 决定要放到哪个 bucket 里面

其实在最后 kv pair 重新组合的时候，我原本采用的操作是直接对原 bucket 进行操作，应该拆分的先 remove 然后 insert 到 image bucket 里面，但是这样线上测试会报内存问题直接零蛋。。很难理解为啥会这样，有空的时候再查查吧

而合并页面的逻辑就会比较清晰一点

* 加表级写锁
* 获取目录和当前 bucket 的数据
* 判断当前 bucket 的深度，如果为 0 就不进行收缩
* 如果当前 bucket 与其 split image bucket 深度不同，说明两个中的一个已经进行拆分了，这时候也不收缩
* 给当前 bucket page 加上读锁，再判断一次是否非空（考虑到并发是有可能存在乱入的情况的），这时候也不收缩
* 这时候才能认为 bucket 已空，真正进行删除操作
* 接着设置原 bucket 的 page为 split image 的 page，即合并 target 和 split
* 最后遍历整个 directory，将所有指向原 bucket page 的 bucket 全部重新指向 split image bucket 
* 通过之前实现的 `CanShrink`判断是否可以收缩 global depth

当然，对于遍历 directory 还是有优化的方式的

```C++
size_t index_diff = 1 << local_depth;
// change bucket_page_id 指向
for (size_t i = target_bucket_index - index_diff; i >= 0 && i < target_bucket_index; i -= index_diff) {
  dir_page->SetBucketPageId(i, image_bucket_page_id);
  dir_page->SetLocalDepth(i, dir_page->GetLocalDepth(target_bucket_index));
}
 
for (size_t i = target_bucket_index + index_diff; i <= dir_page->Size(); i += index_diff) {
  dir_page->SetBucketPageId(i, image_bucket_page_id);
  dir_page->SetLocalDepth(i, dir_page->GetLocalDepth(target_bucket_index));
}
```

通过观察就可以发现，每个 image_index 之间刚好隔着 2^local_depth^的距离，所以只需要遍历这些 index 就好了

> 还有一个很重要的点就是每次使用完一个页面之后记得及时 Unpin，在这上面吃了好多次大亏

顺便说一句，官方给的本地测试实在太弱了

![image-20220428205315360](https://picgo-1306905554.cos.ap-shanghai.myqcloud.com/image-20220428205315360.png)

过了 SampleTest 兴高采烈提交到 GradeScope 就会发现是个大零蛋

需要测试的话还是得靠自己手写 test

或者还用一个非常直球的方法偷官方的 test case（（

```C++
std::ifstream file("/autograder/bustub/test/container/grading_hash_table_scale_test.cpp");
std::string str;
while (file.good()) {
  std::getline(file, str);
  std::cout << str << std::endl;
}
```

不管怎么说，满分万岁（（

真的累死我了写这个 project，写这篇文章也一样（（



![image-20220611212448027](https://picgo-1306905554.cos.ap-shanghai.myqcloud.com/image-20220611212448027.png)

![image-20220611212358763](https://picgo-1306905554.cos.ap-shanghai.myqcloud.com/image-20220611212358763.png)